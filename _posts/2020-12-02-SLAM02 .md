---
layout: post
title: "SLAM02: Non-Linear Least Square Optimization"
---

### First and Second Order Gradient Descend

By approximating the formula as first order or second order talyer series, we can obtain first order or second order (newton method) algorithm respectively. Both of the approaches have their drawbacks

<img src="/assets/img/posts/SLAM_03.png" alt="conversion" class="responsive"/>

### Gaussian Newton
By approximating the objective function f(x) (instead of f(x)^2 )as first order talyor series, we can obtain the expression below:

<img src="/assets/img/posts/SLAM_04.png" alt="conversion" class="responsive"/>

Note that we use J^T J to approximate hessian H. The drawbacks are (1) J^T J is only semi-positive definite (2) the computed delta x could be too large, which makes the approximation incorrect. Both can lead to divergence of the algorithm.

### Levenberg-Marquadt
To alleviate the approximation issue in G-N, the regularization is introduced (also known as lagrange multiplier) to regularize delta x. When the approximate decreases slower than the actual function we believe that the quadratic is a good approximation and hence reduce lambda by a factor of 10, behaving more like gaussian newton. Otherwise, the step is reject, and lambda is increase by a factor of 10, behaving more like first order gradient descend.

<img src="/assets/img/posts/SLAM_05.png" alt="conversion" class="responsive"/>

### Remarks:
1. Deep learning utilize first order algorithm, since gaussian newton does not gaurantee convergence, and Hessian is computationally expensive. 
2. Initialization is crtical for these optimization algorithms. For SLAM, it could ICP or PnP etc.
